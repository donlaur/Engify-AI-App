#!/usr/bin/env tsx
/**
 * Parse Recommendations 11-23 and add to recommendations.json
 * 
 * Converts the provided recommendations text into JSON format
 */

import * as fs from 'fs';
import * as path from 'path';
import { Recommendation, RecommendationsJsonData } from '../../src/lib/workflows/recommendation-schema';

const RECOMMENDATIONS_PATH = path.join(process.cwd(), 'public', 'data', 'recommendations.json');
const BACKUP_RECOMMENDATIONS_PATH = path.join(process.cwd(), 'public', 'data', 'recommendations-backup.json');

// Recommendations 11-23 data (parsed from user input)
const newRecommendations: Omit<Recommendation, 'id'>[] = [
  {
    slug: 'designate-ai-champions-to-scale-adoption',
    title: 'Designate Embedded AI Champions to Scale Adoption and Define Norms',
    description: 'Formalize the role of "AI Champion" within engineering teams to drive behavioral change and scale adoption. AI transformation stalls due to a lack of clear, trusted examples, not a lack of tools. Champions act as embedded, high-trust consultants who make AI\'s value visible, remove friction, and build confidence.',
    recommendationStatement: 'Establish an official AI Champion role, selecting trusted, practical engineers who can act as internal consultants. These individuals should be empowered to define team-specific norms, surface repeatable patterns, and provide peer-to-peer guidance on balancing automation with human judgment.',
    whyThisMatters: 'Access to an AI tool does not guarantee adoption or value. AI transformation is driven by behavioral change, which requires "credibility, context, and consistent examples from people inside the work". Organizations that successfully harness AI are the ones that "overhaul processes, roles, and ways of working", and the AI Champion is the human catalyst for this overhaul. Without this role, adoption remains fragmented, best practices are siloed, and teams fall back on old workflows, failing to realize productivity gains.',
    whenToApply: 'This recommendation should be applied as soon as an organization moves from ad-hoc experimentation to a formal AI adoption strategy. It is particularly critical if: An organization has purchased AI tools (like GitHub Copilot) but is seeing low or inconsistent adoption rates. Engineering managers report that their teams are "stuck" or expressing a "trust deficit" in AI-generated code. There is a desire to scale best practices and prompting techniques from a few power-users to the entire department. This role is effective regardless of team size, but it becomes essential in any organization with more than one or two development teams, where cross-team knowledge sharing becomes a bottleneck.',
    implementationGuidance: 'Do not simply appoint a "manager." Instead, identify and empower organic champions. The best candidates possess a specific set of traits: they are "Strategic" (connecting AI to team goals), "Observant" (spotting opportunities), "Practical" (prioritizing clarity over novelty), and "Trusted" (the person others turn to when stakes are high). Once identified, Champions should be given a formal mandate and the time to execute it. Their key function is to define team norms. This is the practical, cultural implementation of the ai-behavior/trust-but-verify-triage workflow. They should be responsible for: Demonstrating Value: Curating and sharing high-signal examples of AI use that are specific to the team\'s codebase and priorities. Defining Evaluation Standards: Answering the question, "How do we review AI code?" They lead the team in establishing a standard for evaluating AI output and balancing automation with human judgment. Surfacing Patterns: Identifying reusable prompt patterns, templates, or workflows and sharing them (e.g., in a shared prompt library or via a Community of Practice). Acting as a Signal Router: Providing a feedback loop between the team and leadership, surfacing friction, highlighting security or quality risks, and informing the governance/ai-governance-scorecard.',
    relatedWorkflows: ['ai-behavior/trust-but-verify-triage', 'governance/ai-governance-scorecard'],
    relatedGuardrails: [],
    relatedPainPoints: ['pain-point-02-trust-deficit', 'pain-point-04-skill-atrophy', 'pain-point-12-vibe-coding'],
    relatedPrompts: [],
    relatedPatterns: ['persona'],
    researchCitations: [
      {
        source: 'The AI Champion role - Resource | OpenAI Academy',
        summary: 'Defines the AI Champion role as a "high-trust internal consultant" who makes AI\'s value visible and builds confidence.',
        url: 'https://academy.openai.com/public/clubs/champions-ecqup/resources/the-ai-champion-role',
        verified: true,
      },
      {
        source: 'Unlocking the value of AI in software development - McKinsey',
        summary: 'Organizations that successfully harness AI "overhaul processes, roles, and ways of working".',
        url: 'https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/unlocking-the-value-of-ai-in-software-development',
        verified: true,
      },
      {
        source: 'AI Adoption in Software Development: Proven Strategies to Transform Resistant Teams into AI Champions - Tecknoworks',
        summary: 'Effective champions are engineers who were initially skeptical but discovered that AI could free them from "mundane" tasks to focus on higher-value work like "creative architecture and design".',
        url: 'https://tecknoworks.com/ai-adoption-in-software-development/',
        verified: true,
      },
    ],
    primaryKeywords: ['AI champions', 'AI adoption', 'Team structure'],
    recommendationKeywords: ['Define team norms', 'Scale AI adoption', 'Internal consultants', 'Developer productivity'],
    solutionKeywords: ['Behavior change', 'Build trust in AI', 'Knowledge sharing', 'Peer-to-peer guidance'],
    keywords: ['ai', 'team', 'culture', 'governance', 'adoption'],
    category: 'team-structure',
    audience: ['engineering-managers', 'cto', 'vp-engineering'],
    priority: 'high',
    status: 'published',
  },
  {
    slug: 'establish-ai-community-of-practice',
    title: 'Establish an AI Community of Practice (CoP) to Accelerate Innovation',
    description: 'Create a formal, cross-functional AI Community of Practice (CoP) to act as the scaling engine for AI knowledge and governance. While AI Champions (Recommendation 11) operate at the team level, a CoP is the macro-level network that connects them, breaks down organizational silos, and prevents the duplication of effort and tooling.',
    recommendationStatement: 'Establish a formal AI & Data Community of Practice (CoP) with clear governance and leadership. This body should be responsible for developing a "CoP Charter" and "Target Operating Model," facilitating knowledge sharing, and "codifying rules and norms" to accelerate innovation and ensure alignment across the entire organization.',
    whyThisMatters: 'A CoP is a "strategic initiative crucial for fostering a culture of innovation". As teams begin to adopt AI, they will inevitably encounter the same set of problems. Without a CoP, each team solves these problems in isolation, leading directly to pain-point-08-toolchain-sprawl, pain-point-21-duplicate-tooling (e.g., multiple teams building the same internal prompt library), and inconsistent governance. The CoP is the primary mechanism for "breaking down silos" and "enhancing cross-functional communication". It provides a dedicated forum for AI Champions, security experts, data scientists, and legal stakeholders to "leverage the collective intelligence and experience of its staff".',
    whenToApply: 'A CoP should be established once an organization has committed to strategic AI adoption and has multiple teams or individuals (like AI Champions) beginning to generate best practices. It is essential when: An organization needs to scale learnings from a few "power user" teams to the broader company. There is a need to create and maintain a single, organization-wide AI governance policy (governance/ai-governance-scorecard). You observe different teams selecting or building duplicate AI tools, indicating a "toolchain sprawl" problem. The organization includes diverse stakeholders (e.g., legal, security, multiple engineering divisions) who all need input into AI policy.',
    implementationGuidance: 'Establishing an effective CoP is a formal process, not an ad-hoc meeting. Define Purpose and Stakeholders: Clearly articulate the CoP\'s goals, such as "knowledge sharing and skill development in AI". Identify key stakeholders from engineering, security, legal, and product. Establish Leadership and Governance: Form a "core team" responsible for overseeing CoP activities. This team\'s first task is to develop governance structures, including a "CoP Charter," a "Target Operating Model (TOM)," and an "Engagement model". This formalizes the CoP\'s role as the owner of the AI governance framework. Recruit Diverse Members: A CoP must be cross-functional. "Bring everyone to the table," including people from multiple teams, backgrounds, and job titles. Facilitate Collaborative Activities: The CoP should own the central "knowledge base" or platform for AI best practices. It should organize regular activities like workshops, webinars, and "show and tell" sessions to encourage knowledge exchange and celebrate successes. Codify Rules and Norms: The CoP should be responsible for codifying and evolving the "rules and norms" for AI use, including updating the shared process-optimization/structure-your-ai-prompt-library and providing input to the process/platform-consolidation-playbook.',
    relatedWorkflows: ['governance/ai-governance-scorecard', 'process/platform-consolidation-playbook', 'process-optimization/structure-your-ai-prompt-library'],
    relatedGuardrails: [],
    relatedPainPoints: ['pain-point-08-toolchain-sprawl', 'pain-point-21-duplicate-tooling', 'pain-point-12-vibe-coding'],
    relatedPrompts: [],
    relatedPatterns: [],
    researchCitations: [
      {
        source: 'Building an AI and Data Community of Practice | Aim Reply',
        summary: 'A CoP is a "strategic initiative crucial for fostering a culture of innovation".',
        url: 'https://www.reply.com/aim-reply/en/content/methodology-for-an-ai-and-data-community-of-practice-setup',
        verified: true,
      },
      {
        source: 'Why your AI project needs a community of practice and how to build ...',
        summary: 'CoPs "bring everyone to the table" and "leverage the collective intelligence and experience of its staff".',
        url: 'https://stackoverflow.co/teams/resources/why-your-ai-project-needs-a-community-of-practice-and-how-to-build-one/',
        verified: true,
      },
    ],
    primaryKeywords: ['Community of Practice', 'AI governance', 'Team structure'],
    recommendationKeywords: ['Knowledge sharing', 'AI innovation', 'Cross-functional collaboration', 'Scale best practices'],
    solutionKeywords: ['Break down silos', 'Accelerate innovation', 'Reusable knowledge', 'Governance framework'],
    keywords: ['ai', 'cop', 'governance', 'collaboration', 'scaling'],
    category: 'team-structure',
    audience: ['cto', 'vp-engineering', 'engineering-managers'],
    priority: 'medium',
    status: 'published',
  },
  {
    slug: 'implement-ai-literacy-framework',
    title: 'Implement a Formal AI Literacy Framework for All Technical Roles',
    description: 'Implement a formal, multi-level AI literacy framework to build durable skills across the entire organization. AI literacy is a new core competency that is not limited to engineers. It emphasizes critical thinking, ethical reasoning, and the ability to evaluate AI outputs, which are essential skills to mitigate bias, reduce privacy risks, and build resilient, trust-based AI workflows.',
    recommendationStatement: 'Adopt and deploy a structured AI literacy framework to build foundational competence in all technical staff. This framework should define clear competencies across multiple levels, from "Understand" (basic concepts) and "Use" (prompting) to "Analyze & Evaluate" (critical/ethical reflection) and "Create" (building models).',
    whyThisMatters: 'AI literacy is a strategic imperative. Organizations that fail to build it "face bias, privacy risks, and poor decision-making," while organizations that embrace it "achieve agility, innovation, and accountability". This training is not about a specific tool, which will quickly become outdated. Instead, it focuses on "durable skills" like "evaluating outputs, framing problems, and balancing human and machine judgment".',
    whenToApply: 'This recommendation should be implemented immediately as part of any AI adoption initiative. It is a foundational prerequisite for all other recommendations. Apply Level 1 for all employees, including non-technical staff, to establish a common vocabulary. Apply Level 2 as part of the onboarding for any developer AI tool (e.g., Copilot). Apply Level 3 for any engineer or manager responsible for code review, architecture, or team leadership. Apply Level 4 when building an internal AI platform or ML team.',
    implementationGuidance: 'Adopt the four-level pyramid framework for AI literacy, which scaffolds learning from novice to expert: Level 1: Understand AI - Goal: Cover basic terms and concepts. Competencies: Define AI, ML, LLM. Recognize benefits and limitations. Identify different AI types. Understand the role of humans in programming and tuning AI. Level 2: Use and Apply AI - Goal: Achieve fluency in using generative AI tools. Competencies: Utilize prompt engineering techniques. Iterate and collaboratively refine AI outputs. Review content for "hallucinations," incorrect reasoning, and bias. Level 3: Analyze and Evaluate AI - Goal: Critically reflect on AI\'s broader context and implications. Competencies: Analyze ethical considerations (privacy, bias, labor, environment). Critique AI tools and outcomes. Understand how AI\'s lack of context can lead to insecure code. This level is the core requirement for enabling the ai-behavior/trust-but-verify-triage workflow. Level 4: Create AI - Goal: Engage with AI as a creator. Competencies: Build on open APIs. Leverage AI to develop new systems. Propose and build new AI models.',
    relatedWorkflows: ['ai-behavior/trust-but-verify-triage', 'governance/ai-governance-scorecard'],
    relatedGuardrails: ['guardrails/security/prevent-sql-injection-vulnerability'],
    relatedPainPoints: ['pain-point-01-almost-correct-code', 'pain-point-04-skill-atrophy', 'pain-point-19-insecure-code'],
    relatedPrompts: [],
    relatedPatterns: [],
    researchCitations: [
      {
        source: 'AI Literacy White Paper | The Learning and Development Initiative',
        summary: 'AI literacy emphasizes critical thinking, ethical reasoning, and the ability to evaluate AI outputs.',
        url: 'https://ldi.njit.edu/ai-literacy-white-paper',
        verified: true,
      },
      {
        source: 'A Framework for AI Literacy | EDUCAUSE Review',
        summary: 'Four-level pyramid framework for AI literacy: Understand, Use, Analyze & Evaluate, Create.',
        url: 'https://er.educause.edu/articles/2024/6/a-framework-for-ai-literacy',
        verified: true,
      },
      {
        source: 'Understanding Security Risks in AI-Generated Code | CSA',
        summary: 'AI models are "unaware of the risk model behind the code" and optimize for the "shortest path to a passing result," not for security.',
        url: 'https://cloudsecurityalliance.org/blog/2025/07/09/understanding-security-risks-in-ai-generated-code',
        verified: true,
      },
    ],
    primaryKeywords: ['AI literacy', 'AI training', 'Developer skills'],
    recommendationKeywords: ['AI literacy framework', 'Core competency', 'Critical thinking', 'Evaluate AI output'],
    solutionKeywords: ['Mitigate bias', 'Build trust', 'Organizational design', 'Durable skills'],
    keywords: ['ai', 'training', 'literacy', 'education', 'skills'],
    category: 'strategic-guidance',
    audience: ['cto', 'vp-engineering', 'engineering-managers', 'engineers'],
    priority: 'high',
    status: 'published',
  },
  {
    slug: 'mandate-secure-prompt-engineering',
    title: 'Mandate Secure Prompt Engineering Practices for All Developers',
    description: 'Mandate the use of secure prompt engineering practices as the first line of defense in the AI-assisted development lifecycle. The prompt is the new "shift-left"; a vague or naive prompt will predictably generate insecure code, while an explicit, security-aware prompt will produce safer, more robust outputs. This practice is a form of proactive risk mitigation, not just an output-optimization technique.',
    recommendationStatement: 'Develop and enforce a standard for "secure prompt engineering" that all developers must follow. This standard should require prompts to be explicit about security requirements, such as input validation, error handling, data minimization, and the avoidance of hardcoded secrets.',
    whyThisMatters: 'AI-generated code introduces "AI-native" vulnerabilities, the most common of which is the "omission of necessary security controls". This happens because the AI model is "unaware of the risk model behind the code" and optimizes for the "shortest path to a passing result," not for security. For example, a prompt for "user login code" will likely produce code that works but lacks protection against brute-force attacks.',
    whenToApply: 'This practice should be mandated for all developers as soon as they are given access to AI coding assistants. It is a foundational skill that should be taught as part of "Level 2: Use and Apply AI" (from Recommendation 13). Apply this rigor to any prompt that generates new functionality, especially for code handling user input, authentication, data access, or API endpoints. This is particularly critical when working with brownfield code (pain-point-06-brownfield-penalty), where the AI lacks context on existing security patterns.',
    implementationGuidance: 'Develop a Secure Prompt Template: Create a template for common tasks and store it in the shared process-optimization/structure-your-ai-prompt-library. This template should include sections for: Context: (e.g., "This code is for a public-facing API endpoint.") Security Requirements: (e.g., "Must validate all user input per OWASP Top 10. Must use parameterized queries. Must not contain hardcoded secrets.") Data Handling: (e.g., "Prioritize data minimization. Do not log PII.") Dependencies: (e.g., "Use only approved libraries from our internal manifest.") Mandate Task Decomposition: Train developers on the "breakdown" method. For any task larger than a single function, the developer must first instruct the AI to produce a plan (e.g., in a plan.md file). The developer reviews the plan for hallucinations or security oversights before instructing the AI to generate any code. Enforce Approval Gates: Teach developers to include "approval gates" in their prompts, such as "Generate the plan for refactoring this service, then stop and ask for my approval before modifying any files". This keeps the human "in control" and is the best defense against pain-point-03-hallucinated-capabilities.',
    relatedWorkflows: ['ai-behavior/capability-grounding-manifest', 'process-optimization/structure-your-ai-prompt-library', 'security/security-guardrails'],
    relatedGuardrails: ['guardrails/security/prevent-hardcoded-secrets-in-ai-generated-code', 'guardrails/security/prevent-sql-injection-vulnerability'],
    relatedPainPoints: ['pain-point-19-insecure-code', 'pain-point-03-hallucinated-capabilities', 'pain-point-01-almost-correct-code'],
    relatedPrompts: [],
    relatedPatterns: [],
    researchCitations: [
      {
        source: 'Understanding Security Risks in AI-Generated Code | CSA',
        summary: 'AI-generated code introduces "AI-native" vulnerabilities, the most common of which is the "omission of necessary security controls".',
        url: 'https://cloudsecurityalliance.org/blog/2025/07/09/understanding-security-risks-in-ai-generated-code',
        verified: true,
      },
      {
        source: 'Security-Focused Guide for AI Code Assistant Instructions',
        summary: 'Key principles for secure prompts include treating all inputs as untrusted, explicitly forbidding hardcoded secrets, prioritizing data minimization.',
        url: 'https://best.openssf.org/Security-Focused-Guide-for-AI-Code-Assistant-Instructions',
        verified: true,
      },
      {
        source: 'Five Best Practices for Using AI Coding Assistants | Google Cloud Blog',
        summary: 'Adopting a "task decomposition" pattern is critical. Instead of giving AI complex, high-level assignments, developers should "Break down... into several manageable components" and "instruct the AI to ask for your approval before executing on new plan milestones".',
        url: 'https://cloud.google.com/blog/topics/developers-practitioners/five-best-practices-for-using-ai-coding-assistants',
        verified: true,
      },
    ],
    primaryKeywords: ['Secure prompt engineering', 'AI security', 'Prompting best practices'],
    recommendationKeywords: ['Task decomposition', 'Shift-left security', 'Data minimization', 'Security requirements'],
    solutionKeywords: ['Prevent insecure code', 'Avoid hardcoded secrets', 'Proactive risk mitigation', 'Human-in-the-loop'],
    keywords: ['ai', 'prompting', 'security', 'development', 'best-practices'],
    category: 'strategic-guidance',
    audience: ['engineers', 'engineering-managers', 'security', 'cto'],
    priority: 'high',
    status: 'published',
  },
  {
    slug: 'adopt-ai-tool-evaluation-matrix',
    title: 'Adopt a Formal Evaluation Matrix for AI Tool Selection',
    description: 'Implement a formal, matrix-based evaluation process for selecting all AI developer tools. Ad-hoc tool adoption leads to "toolchain sprawl," which creates fragmented workflows, security risks, and escalating costs. A formal matrix moves the decision from a feature-based "beauty contest" to a strategic, trade-off-based analysis aligned with business and security priorities.',
    recommendationStatement: 'Standardize the procurement and adoption of AI coding assistants by using a formal evaluation matrix. This matrix should assess all potential tools against a core set of criteria, including Integration, Security/Privacy, Model Flexibility, Granular Context, and Enterprise Controls, to make a strategic, evidence-based decision.',
    whyThisMatters: 'The choice of an AI coding assistant dictates an organization\'s strategic trade-offs between integration, security, and raw capability. There is no single "best" tool; there is only the best-fit tool for a specific organizational context. For example, a comparative analysis of market leaders reveals three distinct strategic postures: GitHub Copilot: Trades maximum security (it is cloud-only) for seamless integration and "good-enough" quality. Tabnine: Trades model quality (it uses proprietary models) for maximum security (it offers on-prem, air-gapped, permissively-trained models). Cursor: Trades enterprise controls (which are "early-stage") and integration (it\'s a new IDE) for maximum model flexibility (user-configurable models).',
    whenToApply: 'Before procuring the first company-wide AI coding assistant. When organizational pressure to adopt new, unvetted tools (e.g., Cursor) emerges, threatening the existing standard. During any process/platform-consolidation-playbook initiative to provide a neutral, criteria-based framework for the decision. When pain-point-08-toolchain-sprawl has become a recognized problem.',
    implementationGuidance: 'Create a formal "AI Tool Evaluation Matrix" and use it to score all potential vendors. The criteria in this matrix should be synthesized from established frameworks. AI Tool Evaluation Matrix Criteria: Integration & Compatibility: Does it support all team IDEs (VS Code, JetBrains)? Does it support the full tech stack (languages, frameworks)? How disruptive is adoption? (e.g., Plugin vs. new IDE) Security & Privacy: Does it offer on-prem, self-hosted, or air-gapped deployment? What is the data privacy policy? Is prompt data used for training? Does it comply with required regulations (GDPR, HIPAA)? Model Quality & Flexibility: Does it use proprietary, open-source, or closed-source models? Can models be configured or toggled by the user/admin? What is the source of the training data? (e.g., permissively licensed only) Context & Usability: Does it support "Granular Context" (e.g., @-mentions for files, git diff, Jira tickets)? Does it offer "Fast Access to Critical Use Cases" (e.g., one-click test generation, customizable/shareable prompts)? Enterprise Controls & Observability: Does it have robust Role-Based Access Controls (RBAC)? Does it provide "Access to Usage Data" (e.g., adoption metrics, acceptance rates) for observability?',
    relatedWorkflows: ['process/platform-consolidation-playbook', 'governance/ai-governance-scorecard'],
    relatedGuardrails: [],
    relatedPainPoints: ['pain-point-08-toolchain-sprawl', 'pain-point-21-duplicate-tooling', 'pain-point-19-insecure-code'],
    relatedPrompts: [],
    relatedPatterns: [],
    researchCitations: [
      {
        source: 'GitHub Copilot vs. Cursor vs. Tabnine: How to choose the right AI ...',
        summary: 'Comparative analysis of market leaders reveals three distinct strategic postures: Copilot (integration), Cursor (flexibility), Tabnine (security).',
        url: 'https://getdx.com/blog/compare-copilot-cursor-tabnine/',
        verified: true,
      },
      {
        source: 'A framework for evaluating AI code assistants - Continue Blog',
        summary: 'Framework for evaluating AI coding assistants based on integration, security, model flexibility, context, and enterprise controls.',
        url: 'https://blog.continue.dev/a-framework-for-evaluating-ai-code-assistants/',
        verified: true,
      },
    ],
    primaryKeywords: ['AI tool selection', 'AI coding assistants', 'Evaluation matrix'],
    recommendationKeywords: ['GitHub Copilot vs Cursor vs Tabnine', 'Enterprise controls', 'Model flexibility', 'Tool consolidation'],
    solutionKeywords: ['Strategic trade-offs', 'Platform consolidation', 'Security and privacy', 'Prevent toolchain sprawl'],
    keywords: ['ai', 'tools', 'evaluation', 'procurement', 'comparison'],
    category: 'tool-selection',
    audience: ['cto', 'vp-engineering', 'engineering-managers', 'security'],
    priority: 'high',
    status: 'published',
  },
  {
    slug: 'standardize-on-enterprise-ai-tools',
    title: 'Standardize on AI Tools with Enterprise-Grade Security and Privacy Controls',
    description: 'Standardize on a single, approved set of AI developer tools that provide enterprise-grade security and privacy controls. The proliferation of unauthorized "Shadow AI" tools is not merely an inefficiency problem; it is a critical security and compliance failure that exposes proprietary code, customer PII, and company IP to public models.',
    recommendationStatement: 'Consolidate all AI-assisted development onto an approved platform that guarantees data privacy. This includes, at a minimum, contractual assurance that prompt data is not used for model training and, ideally, offers on-premises or private-cloud deployment options to ensure sensitive data never leaves your environment.',
    whyThisMatters: 'The "Toolchain Sprawl" pain point (pain-point-08-toolchain-sprawl) must be reframed as a security incident. When developers use unauthorized public AI tools, the risks are catastrophic. Research shows that 8.5% of employee prompts to public AI tools include sensitive data, such as customer information (46%), employee PII (27%), and legal or financial details (15%). Even more alarming, over half (54%) of these leaks occur on free-tier platforms that explicitly use user queries to train their models.',
    whenToApply: 'This recommendation is urgent and should be applied immediately by any organization that handles sensitive data (e.g., any company with customers). If your organization has not yet provided a paid, enterprise-grade AI tool, developers are using free public tools, and you are leaking data. When executing the process/platform-consolidation-playbook. As a core requirement for the governance/ai-governance-scorecard.',
    implementationGuidance: 'Perform an Audit: Identify all "Shadow AI" tools currently being used by developers. Execute the Evaluation Matrix (Rec 15): Use the formal matrix to select a single, standard platform. Prioritize the "Security & Privacy" and "Enterprise Controls" criteria. Look for vendors (like Tabnine) that offer on-prem/air-gapped options or vendors (like Microsoft) that offer strong contractual privacy guarantees via their enterprise stack. Procure and Deploy: Provide the approved tool to all developers. The cost of enterprise seats is negligible compared to the cost of a single PII data breach. Block Unauthorized Tools: Implement technical controls (see Recommendation 21) to block access to unapproved AI tools at the network level. Train and Consolidate: Aggressively evangelize the use of the new, standard tool. This is a key task for AI Champions (Recommendation 11) and the CoP (Recommendation 12). Frame it as a move that enables developers to use AI safely.',
    relatedWorkflows: ['process/platform-consolidation-playbook', 'governance/ai-governance-scorecard', 'security/security-guardrails'],
    relatedGuardrails: [],
    relatedPainPoints: ['pain-point-08-toolchain-sprawl', 'pain-point-21-duplicate-tooling', 'pain-point-19-insecure-code'],
    relatedPrompts: [],
    relatedPatterns: [],
    researchCitations: [
      {
        source: 'Protecting Sensitive Data in the Age of Generative AI: Risks, Challenges, and Solutions',
        summary: '8.5% of employee prompts to public AI tools include sensitive data, such as customer information (46%), employee PII (27%), and legal or financial details (15%).',
        url: 'https://www.kiteworks.com/cybersecurity-risk-management/sensitive-data-ai-risks-challenges-solutions/',
        verified: true,
      },
      {
        source: 'Shadow AI & Data Leakage: How to Secure Gen AI at Work',
        summary: 'Over half (54%) of data leaks occur on free-tier platforms that explicitly use user queries to train their models.',
        url: 'https://versa-networks.com/blog/shadow-ai-data-leakage-how-to-secure-generative-ai-at-work/',
        verified: true,
      },
    ],
    primaryKeywords: ['AI data privacy', 'Enterprise AI security', 'AI tool standardization'],
    recommendationKeywords: ['Data leakage', 'PII', 'Shadow AI', 'Proprietary code'],
    solutionKeywords: ['Platform consolidation', 'Enterprise controls', 'Data governance', 'On-premises AI'],
    keywords: ['ai', 'security', 'privacy', 'governance', 'data', 'tools'],
    category: 'tool-selection',
    audience: ['cto', 'vp-engineering', 'security', 'legal'],
    priority: 'high',
    status: 'published',
  },
  {
    slug: 'integrate-ai-analysis-in-cicd',
    title: 'Integrate AI-Powered Analysis into CI/CD Pipelines for Quality Assurance',
    description: 'Embed AI-powered static analysis and security tools directly into the CI/CD pipeline. This creates a non-negotiable, automated governance layer that validates all code, including AI-generated code, before it can be merged. This moves AI from being just a generator of code to a validator of quality and security.',
    recommendationStatement: 'Integrate AI-augmented static analysis tools into your CI/CD pipeline to automatically scan all pull requests for quality issues, security vulnerabilities, and hardcoded secrets. Adopt tools that use AI for advanced analysis (like taint analysis) and to provide "AI-powered remediation" suggestions directly in the developer workflow.',
    whyThisMatters: 'The CI/CD pipeline is the organization\'s ultimate quality and security gate. It is the automated backstop that catches flaws missed by human processes, such as code review (Recommendation 20) or secure prompting (Recommendation 14). This is critical because AI-generated code can look plausible but contain subtle, "AI-native" vulnerabilities like the "omission of necessary security controls".',
    whenToApply: 'This should be a standard, non-negotiable part of all CI/CD pipelines in an organization, especially for projects that are high-risk or have high-velocity-AI code generation. Apply this when you need to automate the enforcement of the governance/ai-governance-scorecard. This is the technical solution for organizations that are seeing pain-point-01-almost-correct-code or pain-point-19-insecure-code slip past human reviewers.',
    implementationGuidance: 'Select an AI-Augmented SAST Tool: Choose a tool that offers deep security analysis and pipeline integration (e.g., SonarQube, Semgrep, Codacy). Prioritize tools that explicitly mention AI-powered remediation or "AI CodeFix". Configure Pipeline Integration: Set up the tool to automatically scan all new pull requests and branches. The scan should be a required check that must pass before a PR can be merged. Enable Advanced Checks: Do not settle for basic linting. Enable the most valuable analysis features: Taint Analysis: To find data-flow vulnerabilities like SQL injection. Secrets Detection: To prevent API keys and passwords from being committed. SCA (Software Composition Analysis): To detect insecure, outdated, or (in the case of hallucinations) non-existent dependencies. Enable AI-Powered Remediation: Activate features like "AI CodeFix". This provides immediate, actionable fix suggestions directly in the developer\'s workflow (e.g., as a PR comment), which streamlines remediation and acts as a powerful teaching tool. Tune and Iterate: Use the governance/ai-governance-scorecard to define which rules are blockers (high severity) versus which are warnings (medium/low).',
    relatedWorkflows: ['governance/ai-governance-scorecard', 'security/security-guardrails', 'process/release-readiness-runbook'],
    relatedGuardrails: ['guardrails/security/prevent-sql-injection-vulnerability', 'guardrails/security/prevent-hardcoded-secrets-in-ai-generated-code', 'guardrails/data-integrity/prevent-data-corruption-in-ai-generated-migrations'],
    relatedPainPoints: ['pain-point-19-insecure-code', 'pain-point-01-almost-correct-code', 'pain-point-16-guardrail-evasion'],
    relatedPrompts: [],
    relatedPatterns: [],
    researchCitations: [
      {
        source: 'Code Quality & Security Software | Static Analysis Tool | Sonar',
        summary: 'SonarQube provides AI-powered remediation suggestions directly in the developer workflow.',
        url: 'https://www.sonarsource.com/products/sonarqube/',
        verified: true,
      },
      {
        source: 'Codacy - Enterprise-Grade Security for AI-Accelerated Coding',
        summary: 'Codacy offers AI-augmented static analysis with taint analysis and secrets detection.',
        url: 'https://www.codacy.com/',
        verified: true,
      },
    ],
    primaryKeywords: ['AI in CI/CD', 'Automated code quality', 'AI-powered remediation'],
    recommendationKeywords: ['Static Analysis (SAST)', 'SonarQube AI CodeFix', 'Taint analysis', 'Secrets detection'],
    solutionKeywords: ['Automated governance', 'Quality gates', 'Continuous validation', 'Secure pipeline'],
    keywords: ['ai', 'cicd', 'devops', 'security', 'quality', 'automation'],
    category: 'process-optimization',
    audience: ['devops-sre', 'engineering-managers', 'security', 'cto'],
    priority: 'high',
    status: 'published',
  },
  {
    slug: 'use-ai-for-test-generation-and-maintenance',
    title: 'Use AI for Automated Test Generation and Self-Healing Maintenance',
    description: 'Integrate AI into the quality assurance (QA) process to automatically generate test cases, optimize test suites, and perform "self-healing" maintenance on brittle automation scripts. This moves AI\'s role in testing beyond simple TDD (Recommendation 7) and uses it to solve the significant economic and time costs of test maintenance.',
    recommendationStatement: 'You should leverage AI testing solutions to automatically generate test cases for new code, identify and cover gaps in existing test coverage, and dynamically select which tests to run. Critically, adopt tools with "self-healing" capabilities to automatically fix broken tests, reducing the high cost of test suite maintenance.',
    whyThisMatters: 'While "Test-Driven Development with AI" (Recommendation 7) is a valuable defensive practice, this recommendation is the offensive counterpart. It uses AI to actively improve and maintain the quality of the test suite itself. The economic viability of test automation is often destroyed by maintenance costs, particularly for UI tests where "tests... break with every UI change". AI-powered "self-healing" capabilities directly address this pain point by automatically detecting and fixing broken UI locators and script elements, which "minimized test maintenance".',
    whenToApply: 'This is highly applicable to teams that have a large, existing test automation suite (especially for UI) that is "flaky" or expensive to maintain. When development teams are struggling to achieve test coverage goals or are frequently surprised by edge-case bugs in production. In CI/CD pipelines where long-running test suites have become a significant bottleneck.',
    implementationGuidance: 'Assess and Integrate: Start by assessing your existing CI/CD tools and test frameworks for their ability to integrate with AI testing solutions via APIs or plugins. Start Small: Select a small, manageable module or a single application to introduce AI-based testing. Use this to train the AI model on your historical test results, production logs, and defect data. Prioritize Self-Healing: Focus first on implementing a "self-healing" solution for your most brittle test suite (e.g., Selenium, Playwright). This will provide the fastest and most visible ROI by reducing maintenance overhead. Enable Test Generation: Use AI tools to analyze your codebase and "identify previously untested application areas". Generate new test cases to address these gaps, and have QA engineers review and approve them in a "human-in-the-loop" process. Optimize the Pipeline: Once the model is trained, enable "dynamic test selection" in your CI/CD pipeline to intelligently shorten build times, and use "predictive defect detection" to flag high-risk PRs for more intensive review.',
    relatedWorkflows: ['code-quality/tdd-with-ai-pair', 'process/release-readiness-runbook'],
    relatedGuardrails: ['guardrails/testing/prevent-missing-edge-case-tests'],
    relatedPainPoints: ['pain-point-01-almost-correct-code', 'pain-point-05-missing-context'],
    relatedPrompts: [],
    relatedPatterns: [],
    researchCitations: [
      {
        source: 'Testing AI Code in CI/CD Made Simple for Developers - Speedscale',
        summary: 'AI-powered "self-healing" capabilities automatically detect and fix broken UI locators and script elements, which "minimized test maintenance".',
        url: 'https://speedscale.com/blog/testing-ai-code-in-cicd-made-simple-for-developers/',
        verified: true,
      },
      {
        source: 'How to Integrate AI Testing into Your CI/CD Pipeline - QASource Blog',
        summary: 'AI can analyze application changes, historical data, and user behavior patterns to "automatically create test scenarios that human testers might overlook".',
        url: 'https://blog.qasource.com/software-development-and-qa-tips/how-to-integrate-ai-testing-solution-into-ci-cd-pipeline',
        verified: true,
      },
    ],
    primaryKeywords: ['AI test automation', 'AI in QA', 'Test generation'],
    recommendationKeywords: ['Self-healing automation', 'Dynamic test selection', 'Predictive defect detection', 'Test maintenance'],
    solutionKeywords: ['Reduce flaky tests', 'Automated test generation', 'CI/CD optimization', 'Quality assurance'],
    keywords: ['ai', 'testing', 'qa', 'automation', 'cicd', 'maintenance'],
    category: 'process-optimization',
    audience: ['qa', 'devops-sre', 'engineers', 'engineering-managers'],
    priority: 'medium',
    status: 'published',
  },
  {
    slug: 'leverage-ai-for-pr-summaries-release-notes',
    title: 'Leverage Generative AI to Automate PR Summaries and Release Notes',
    description: 'Automate the creation of PR/MR descriptions, code change summaries, and release notes by integrating generative AI into the CI/CD pipeline. This "beyond the IDE" use case leverages AI to analyze git diffs and automate the time-consuming documentation and communication tasks that surround code changes, reducing cognitive load for both authors and reviewers.',
    recommendationStatement: 'Configure your CI/CD pipeline to automatically trigger a generative AI job on every pull request. This job should analyze the code changes (git diff) and generate a "Summary of the changes," "PR/MR comments for initial feedback," and a draft of "Release Notes".',
    whyThisMatters: 'This recommendation directly complements "Enforce Small PRs" (Recommendation 4) by solving the human bottleneck of code review. A small PR (code-quality/keep-prs-under-control) is useless if it sits in a review queue for days because reviewers lack the cognitive context to start. An AI-generated "Summary of the changes" provides this context instantly, reducing time-to-review and accelerating the entire development cycle.',
    whenToApply: 'This is a high-value optimization for any team that struggles with slow code review cycles or inconsistent documentation. It is a perfect complement to teams that have successfully adopted the code-quality/keep-prs-under-control workflow, as it optimizes the review step that follows. Apply this when product managers or technical writers struggle to keep up with the pace of engineering, and release notes are often a bottleneck.',
    implementationGuidance: 'Choose a GenAI Integration Point: This can be implemented using a tool like the friendly-cicd-helper demonstrated by Google or by writing a custom script that runs in your CI/CD pipeline (e.g., GitHub Actions, GitLab CI). Configure the CI Job: Create a new job in your CI configuration (e.g., cloudbuild.yaml) that is triggered on a new merge request or pull request. Use a Git Diff as Context: The script should run a git diff to capture the code changes. This diff will be the primary context fed to the generative AI model. Prompt the AI Model: The script will call a generative AI model (e.g., via the Vertex AI API) with a specific prompt. The prompt should ask for three distinct outputs: "Generate a high-level summary of these code changes for a pull request description." "Review these changes and generate initial code review comments for the author." "Based on these changes, generate a draft of release notes suitable for a product manager." Post-Back to the PR: The CI/CD job should then post these outputs back to the PR as a comment, update the PR description, or (in the case of release notes) save them as an artifact or send them to a tool like Jira.',
    relatedWorkflows: ['code-quality/keep-prs-under-control', 'process/release-readiness-runbook'],
    relatedGuardrails: [],
    relatedPainPoints: ['pain-point-10-oversized-prs', 'pain-point-05-missing-context', 'pain-point-23-tactical-trap'],
    relatedPrompts: [],
    relatedPatterns: [],
    researchCitations: [
      {
        source: 'Boost your Continuous Delivery pipeline with Generative AI | Google ...',
        summary: 'Demonstrates using generative AI to analyze git diffs and generate PR summaries, code review comments, and release notes.',
        url: 'https://cloud.google.com/blog/topics/developers-practitioners/boost-your-continuous-delivery-pipeline-with-generative-ai',
        verified: true,
      },
    ],
    primaryKeywords: ['AI-generated release notes', 'Automated PR summaries', 'AI in CI/CD'],
    recommendationKeywords: ['Git diff analysis', 'Automated documentation', 'Code review automation', 'Generative AI'],
    solutionKeywords: ['Reduce cognitive load', 'Accelerate code review', 'Process optimization', 'Developer productivity'],
    keywords: ['ai', 'cicd', 'automation', 'pr', 'release-notes', 'documentation'],
    category: 'process-optimization',
    audience: ['engineers', 'engineering-managers', 'devops-sre', 'cto'],
    priority: 'medium',
    status: 'published',
  },
  {
    slug: 'augment-code-review-with-ai-checklist',
    title: 'Augment Code Reviews with an AI-Specific Validation Checklist',
    description: 'Update all code review standards to include a mandatory checklist for "AI-native" vulnerabilities. Traditional checklists are necessary but insufficient, as they are not designed to catch the subtle, context-deficient flaws that AI-generated code introduces. This augmentation is essential to maintain code quality and security in an AI-assisted environment.',
    recommendationStatement: 'Augment your team\'s existing pull request (PR) checklists with a new, required section for "AI-Specific Validation." This new checklist should force the human reviewer to pause and explicitly check for common AI-generated flaws, such as omission of security controls, subtle logic errors, and "hallucinated" dependencies.',
    whyThisMatters: 'This is the single most important human-in-the-loop defense for pain-point-01-almost-correct-code. AI models are "unaware of your application\'s risk model" and are incentivized to find the "shortest path to a passing result," which leads to a new class of "AI-native" vulnerabilities. These flaws are dangerous because the code looks plausible and often passes basic checks, but it contains critical flaws, such as: Omission of Security Controls: The AI forgets to add input validation, sanitization, or authorization checks because they were not explicitly in the prompt. Subtle Logic Errors: The AI introduces a bug that looks correct, such as using if user.role == "admin" (which fails for multi-role users) instead of if "admin" in user.roles (which is correct). Optimization Shortcuts: The AI uses a dangerous but functional shortcut, like eval(expression), which solves the prompt but opens a remote code execution vulnerability. Hallucinated Dependencies: The AI "invents" a package name. An attacker can then register this package name ("slopsquatting") and publish malicious code, which your developer then installs. Architectural Drift: The AI non-deterministically swaps a critical library (e.g., a cryptography library) or removes an access control check, breaking security invariants.',
    whenToApply: 'This augmented checklist should be a mandatory part of every code review for any team that uses AI coding assistants. It is not optional. It should be physically added to the pull request template in your code host (e.g., GitHub, GitLab). This is especially critical for PRs that are "AI-heavy" or touch critical code paths (auth, payments, data migrations).',
    implementationGuidance: 'Augment your existing PR checklist with a new, mandatory "AI-Specific Validation" section. Part 1: Standard Code Review Checklist - [ ] Functionality: Does the code meet all requirements and handle edge cases? [ ] Readability & Style: Does it follow team coding standards? [ ] Design: Does it follow established architectural patterns? [ ] Performance: Does it introduce any bottlenecks? [ ] Error Handling: Are errors handled gracefully? [ ] Testing: Are there sufficient unit and integration tests? (See Rec 7) [ ] Documentation: Is the code (and PR) adequately documented? (See Rec 19) Part 2: MANDATORY AI-Specific Validation Checklist - [ ] Omission Check: What security controls is this code missing? (Check for input validation, output encoding, and authorization.) [ ] Logic Check: Is there a subtle logic error? (Check logic, e.g., == vs. in, or off-by-one.) [ ] Dependency Check: Are all new packages real, secure, and approved? (Check for "hallucinated dependencies".) [ ] Context Check: Did the AI take a dangerous "shortcut" (like eval()) that violates our security posture? [ ] Drift Check: Did the AI change any existing security-critical code (e.g., auth, crypto) that was outside the PR\'s main scope? Finally, this review process must create a feedback loop. When a reviewer finds a common AI mistake, they should "Document Common AI Mistakes" and "Refine Your Prompts". This finding should be given to the AI Champion (Rec 11) and shared in the CoP (Rec 12) to update the central process-optimization/structure-your-ai-prompt-library.',
    relatedWorkflows: ['ai-behavior/trust-but-verify-triage', 'code-quality/tdd-with-ai-pair', 'process-optimization/structure-your-ai-prompt-library'],
    relatedGuardrails: ['guardrails/security/prevent-sql-injection-vulnerability', 'guardrails/testing/prevent-missing-edge-case-tests'],
    relatedPainPoints: ['pain-point-01-almost-correct-code', 'pain-point-19-insecure-code', 'pain-point-03-hallucinated-capabilities'],
    relatedPrompts: [],
    relatedPatterns: [],
    researchCitations: [
      {
        source: 'The Most Common Security Vulnerabilities in AI-Generated Code ...',
        summary: 'AI models are "unaware of your application\'s risk model" and are incentivized to find the "shortest path to a passing result," which leads to "AI-native" vulnerabilities.',
        url: 'https://www.endorlabs.com/learn/the-most-common-security-vulnerabilities-in-ai-generated-code',
        verified: true,
      },
      {
        source: 'How to Review AI-Generated Code: A Guide for Developers - Arsturn',
        summary: 'AI-specific validation checklist for code reviews, including checks for omitted security controls, subtle logic errors, and hallucinated dependencies.',
        url: 'https://www.arsturn.com/blog/the-essential-guide-to-reviewing-ai-generated-code',
        verified: true,
      },
    ],
    primaryKeywords: ['AI code review', 'Code review checklist', 'AI-native vulnerabilities'],
    recommendationKeywords: ['AI-Specific Validation', 'Omission of security controls', 'Subtle logic errors', 'Hallucinated dependencies'],
    solutionKeywords: ['Trust but verify', 'Human-in-the-loop', 'Secure code review', 'Quality assurance'],
    keywords: ['code review', 'quality', 'security', 'validation', 'checklist', 'ai'],
    category: 'best-practices',
    audience: ['engineers', 'engineering-managers', 'qa', 'security'],
    priority: 'high',
    status: 'published',
  },
  {
    slug: 'implement-dlp-and-genai-firewalls',
    title: 'Implement Data Loss Prevention (DLP) and "GenAI Firewalls" for AI Tools',
    description: 'Implement dedicated Data Loss Prevention (DLP) solutions and "Generative AI Firewalls" to provide technical, real-time enforcement against data exfiltration via AI prompts. Policies and training (Rec 22) are insufficient to mitigate the risk of "Shadow AI"; organizations must deploy technical controls to monitor and block sensitive data from leaving the network.',
    recommendationStatement: 'Deploy a "GenAI Firewall" or equivalent DLP solution that is capable of monitoring and controlling all generative AI traffic. This solution must be configured to perform real-time content inspection, detect and block sensitive data (PII, secrets, proprietary code) in prompts, and enforce policies that block unauthorized AI tools.',
    whyThisMatters: 'While standardizing on approved tools (Recommendation 16) and governing data use (Recommendation 22) are critical policies, they are ineffective without technical enforcement. Developers will inevitably use the path of least resistance, which may include pasting sensitive data into unauthorized "Shadow AI" tools. A "Generative AI Firewall" is the only reliable, automated solution to this problem. It is the "security guardrail" for prompts and network traffic.',
    whenToApply: 'This recommendation should be implemented by any organization that: Handles any PII, financial data (PCI), or health data (HIPAA). Has proprietary source code or intellectual property that is a core business asset. Has standardized on an enterprise AI tool (Rec 16) and now needs to enforce that standard by blocking all others. Is building out its security/security-guardrails and recognizes the prompt as a new, ungoverned attack surface.',
    implementationGuidance: 'Assess Network/DLP Capabilities: Evaluate your existing network firewall and DLP solutions. They may already have "GenAI" capabilities that can be enabled. Evaluate GenAI Firewall Vendors: If your existing tools are insufficient, evaluate dedicated GenAI firewall vendors. Use the "Security & Privacy" criteria from the AI Tool Evaluation Matrix (Rec 15). Define and Implement Policies: Work with the cross-functional AI CoP (Rec 12) and Legal (Rec 22) to define policies. These should include: Blocklist: A list of all unauthorized AI tools to be blocked at the network level. Allowlist: The approved AI tools and endpoints (e.g., your enterprise GitHub tenant). Content Policies: DLP rules that inspect outbound traffic to the allowlist, looking for patterns that match PII, API keys, or proprietary code markers. Configure Actions: Define actions for policy violations: Block and Alert: For high-severity violations (e.g., PII in a prompt), block the request and send a high-priority alert to the security team. Log: For sanctioned tools, log all interactions to provide an audit trail for compliance. Develop an Incident Response Plan: Create a specific playbook for "AI data leakage" incidents. What is the process when the firewall blocks a user? How is this escalated? This plan is a key part of your AI governance.',
    relatedWorkflows: ['security/security-guardrails', 'process/platform-consolidation-playbook', 'governance/ai-governance-scorecard'],
    relatedGuardrails: [],
    relatedPainPoints: ['pain-point-08-toolchain-sprawl', 'pain-point-12-vibe-coding', 'pain-point-19-insecure-code'],
    relatedPrompts: [],
    relatedPatterns: [],
    researchCitations: [
      {
        source: 'How to Prevent Generative AI Data Leakage - Zscaler',
        summary: 'GenAI Firewall solutions can monitor and control all generative AI traffic, detect and block sensitive data in prompts, and enforce policies that block unauthorized AI tools.',
        url: 'https://www.zscaler.com/blogs/product-insights/how-to-prevent-generative-ai-data-leakage',
        verified: true,
      },
    ],
    primaryKeywords: ['GenAI Firewall', 'Data Loss Prevention (DLP)', 'AI data security'],
    recommendationKeywords: ['Shadow AI', 'Data exfiltration', 'Real-time content inspection', 'Block unauthorized AI'],
    solutionKeywords: ['Technical enforcement', 'Network security', 'AI Incident Response Plan', 'Data governance'],
    keywords: ['ai', 'security', 'dlp', 'firewall', 'network', 'governance'],
    category: 'risk-mitigation',
    audience: ['security', 'devops-sre', 'cto', 'legal'],
    priority: 'high',
    status: 'published',
  },
  {
    slug: 'enforce-governance-over-sensitive-data-in-prompts',
    title: 'Enforce Strict Governance Over Sensitive Data and PII in AI Prompts',
    description: 'Establish a cross-functional governance framework, co-owned by Legal and Engineering, to manage the high risk of sensitive data being exposed to AI tools. This is not just an engineering policy; it is a core business and legal strategy to prevent data breaches, compliance failures, and the loss of intellectual property.',
    recommendationStatement: 'Create and enforce a clear, organization-wide data governance policy specifically for AI systems. This policy must explicitly forbid entering any sensitive data (customer PII, financial data, health data, proprietary source code) into any public or non-enterprise-sandboxed AI tool. This policy must be co-owned by Legal, HR, and IT.',
    whyThisMatters: 'The "prompt" is a new, unsecured vector for catastrophic data loss. The risk is not hypothetical; it is active and ongoing. Research shows that 8.5% of employee prompts to generative AI tools contain sensitive data. This includes customer information (46%), employee PII (27%), and legal or financial details (15%). Over half (54%) of these leaks are to free-tier platforms that explicitly use user data to train their models. The consequences are severe: Compliance Failure: Leaking customer PII is a direct violation of regulations like GDPR, HIPAA, and CCPA, leading to massive fines. IP Loss: Leaking proprietary source code or product roadmaps to a public model effectively "donates" your core intellectual property to your competitors. Erosion of Trust: A public data breach involving AI tools can destroy customer and market trust.',
    whenToApply: 'This policy must be in place before any developer is given access to any AI tool. This is a foundational, Day 0 requirement for any organization, especially those in regulated industries (finance, healthcare, defense). This policy should be reviewed and signed by all new hires as part of the onboarding process.',
    implementationGuidance: 'Form a Cross-Functional Team: The CTO must initiate a meeting with the General Counsel, CISO, and head of HR to "designate cross-functional AI leads". This team will co-own the AI governance policy. Define "Sensitive Data": The policy must be unambiguous. Clearly define what "sensitive data" means for your organization (e.g., "Any data that is not public," "All customer PII," "All source code not explicitly open-sourced"). Establish a Clear Policy: The policy should be simple and absolute: "You must not enter sensitive data into any AI tool that is not the company-approved, enterprise-sandboxed platform." Implement Technical Controls (Rec 16, 21): Preventive: Provide a safe alternative. Procure and standardize on an enterprise-grade tool (Rec 16) that guarantees data privacy. Detective: Implement auditing tools (like Microsoft Purview) to monitor AI interactions and manage compliance. Blocking: Implement a GenAI Firewall (Rec 21) to technically block sensitive data from leaving the network. Mandate Training (Rec 13): All employees must complete "AI literacy" (Rec 13) and "AI-Specific Security Awareness Training" (Rec 18) that explicitly covers this data governance policy. Enforce Secure Prompting (Rec 14): Train developers on "data minimization" as a core prompt engineering practice.',
    relatedWorkflows: ['governance/ai-governance-scorecard', 'security/security-guardrails'],
    relatedGuardrails: ['guardrails/security/prevent-hardcoded-secrets-in-ai-generated-code'],
    relatedPainPoints: ['pain-point-19-insecure-code', 'pain-point-12-vibe-coding', 'pain-point-08-toolchain-sprawl'],
    relatedPrompts: [],
    relatedPatterns: [],
    researchCitations: [
      {
        source: 'Microsoft Purview data security and compliance protections for generative AI apps',
        summary: 'Microsoft Purview provides auditing tools to monitor AI interactions and manage compliance.',
        url: 'https://learn.microsoft.com/en-us/purview/ai-microsoft-purview',
        verified: true,
      },
      {
        source: 'AI and Machine Learning in Sensitive Data Management - PII Tools',
        summary: '8.5% of employee prompts to generative AI tools contain sensitive data, including customer information (46%), employee PII (27%), and legal or financial details (15%).',
        url: 'https://pii-tools.com/ai-in-sensitive-data-management/',
        verified: true,
      },
    ],
    primaryKeywords: ['AI data governance', 'Sensitive data PII', 'AI risk mitigation'],
    recommendationKeywords: ['Proprietary code', 'Intellectual property', 'Compliance (GDPR, HIPAA)', 'Cross-functional governance'],
    solutionKeywords: ['Data Loss Prevention', 'Data minimization', 'AI policy', 'Legal and compliance'],
    keywords: ['ai', 'governance', 'data', 'security', 'legal', 'pii', 'compliance'],
    category: 'risk-mitigation',
    audience: ['legal', 'cto', 'vp-engineering', 'security'],
    priority: 'high',
    status: 'published',
  },
  {
    slug: 'mitigate-ip-and-copyright-risks-from-ai',
    title: 'Mitigate Intellectual Property (IP) and Copyright Risks from AI-Generated Code',
    description: 'Proactively mitigate the legal and intellectual property (IP) risks associated with AI-generated code. Models trained on public repositories may generate code that is "derivative" of existing copyrighted or copyleft-licensed software, creating a "copyright infringement" risk. This could inadvertently "stain" a proprietary codebase with a restrictive license (e.g., GPL), creating a significant legal and business liability.',
    recommendationStatement: 'Establish a formal strategy, in partnership with Legal, to mitigate the IP and copyright risks of AI-generated code. This strategy must include: 1) prioritizing AI tools that provide "legal clarity" on their training data, 2) enforcing Software Composition Analysis (SCA) in the CI/CD pipeline to check for license compliance, and 3) conducting a legal review of tool-vendor indemnity policies.',
    whyThisMatters: 'There are two primary IP risks with AI-generated code: Copyright Infringement (Inbound): AI models trained on public repositories (which include GPL, AGPL, and other restrictive licenses) "can assemble code that is very similar to existing software without adhering to licensing terms". If your developer accepts this code, your organization\'s proprietary product may now be a "derivative work" of a copyleft-licensed project, creating a massive legal obligation you cannot meet. Data Leakage (Outbound): As described in Rec 22, if your own "sensitive information included in training data (such as proprietary codebases) may be reproduced in generated AI outputs" for other companies, you are actively leaking your own IP.',
    whenToApply: 'This is a mandatory consideration for any organization that builds and sells proprietary, closed-source software. This risk analysis must be performed during the tool selection process (Rec 15). This should be reviewed annually with the Legal department as part of the governance/ai-governance-scorecard review.',
    implementationGuidance: 'This is a three-part mitigation strategy: Prioritize Tools with "Legal Clarity": This risk must be a central criterion in your Tool Selection Matrix (Rec 15). The "Model Provider" and "Training Data Source" criteria are not just about quality; they are about legal indemnity. Strongly consider tools (like Tabnine) that are "purpose-built for enterprises" and whose "proprietary models [are] trained exclusively on permissively licensed open source". This "legal clarity" is their primary value proposition. Conversely, for tools (like Copilot) that are trained more broadly, your Legal team must review their indemnity policies and determine if the organization accepts the ambiguity. Enforce Technical Guardrails (Rec 17): The AI-augmented CI/CD pipeline (Rec 17) must include a robust Software Composition Analysis (SCA) scanner. This scanner is the technical guardrail that checks for both security vulnerabilities in dependencies and license compliance, flagging any code (AI-generated or not) that introduces a restrictive license. Partner with Legal: The "cross-functional AI leads" (Rec 22) must include Legal. Legal must review and approve the selected AI tool and its terms of service. Legal must help define the SCA policies (e.g., "Block all GPL/AGPL licenses"). Legal must be involved in the governance/ai-governance-scorecard to assess the organization\'s overall risk tolerance for code-origin ambiguity.',
    relatedWorkflows: ['process/platform-consolidation-playbook', 'governance/ai-governance-scorecard', 'process/release-readiness-runbook'],
    relatedGuardrails: ['guardrails/security/prevent-sql-injection-vulnerability'],
    relatedPainPoints: ['pain-point-08-toolchain-sprawl', 'pain-point-19-insecure-code'],
    relatedPrompts: [],
    relatedPatterns: [],
    researchCitations: [
      {
        source: 'AI-generated Code: How to Protect Your Software From AI ...',
        summary: 'AI models trained on public repositories "can assemble code that is very similar to existing software without adhering to licensing terms".',
        url: 'https://www.ox.security/blog/ai-generated-code-how-to-protect-your-software-from-ai-generated-vulnerabilities/',
        verified: true,
      },
      {
        source: 'Managing Data Security and Privacy Risks in Enterprise AI | Frost Brown Todd',
        summary: 'If your own "sensitive information included in training data (such as proprietary codebases) may be reproduced in generated AI outputs" for other companies, you are actively leaking your own IP.',
        url: 'https://frostbrowntodd.com/managing-data-security-and-privacy-risks-in-enterprise-ai/',
        verified: true,
      },
      {
        source: 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile - NIST Technical Series Publications',
        summary: 'NIST framework for managing AI risks, including IP and copyright risks from AI-generated code.',
        url: 'https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf',
        verified: true,
      },
    ],
    primaryKeywords: ['AI IP risk', 'AI copyright', 'Intellectual property'],
    recommendationKeywords: ['License compliance', 'Copyleft', 'GPL', 'Derivative work'],
    solutionKeywords: ['Software Composition Analysis (SCA)', 'Legal clarity', 'Permissive-licensed training', 'Legal indemnity'],
    keywords: ['ai', 'legal', 'ip', 'copyright', 'license', 'sca', 'governance'],
    category: 'risk-mitigation',
    audience: ['legal', 'cto', 'vp-engineering', 'security'],
    priority: 'medium',
    status: 'published',
  },
];

async function main() {
  // Read existing recommendations
  const existingData: RecommendationsJsonData = JSON.parse(
    fs.readFileSync(RECOMMENDATIONS_PATH, 'utf-8')
  );

  // Create backup
  fs.writeFileSync(BACKUP_RECOMMENDATIONS_PATH, JSON.stringify(existingData, null, 2));

  // Add new recommendations with IDs
  const newRecommendationsWithIds = newRecommendations.map((rec, index) => ({
    ...rec,
    id: `recommendation-${String(index + 11).padStart(2, '0')}-${rec.slug}`,
  }));

  // Merge with existing recommendations
  const updatedRecommendations = [
    ...existingData.recommendations,
    ...newRecommendationsWithIds,
  ];

  // Update the JSON data
  const updatedData: RecommendationsJsonData = {
    ...existingData,
    totalRecommendations: updatedRecommendations.length,
    recommendations: updatedRecommendations,
    generatedAt: new Date().toISOString(),
  };

  // Write updated recommendations
  fs.writeFileSync(RECOMMENDATIONS_PATH, JSON.stringify(updatedData, null, 2));

  console.log(` Added ${newRecommendations.length} new recommendations (11-23)`);
  console.log(`   Total recommendations: ${updatedData.totalRecommendations}`);
  console.log(`   Backup created: ${BACKUP_RECOMMENDATIONS_PATH}`);
}

main().catch(console.error);

